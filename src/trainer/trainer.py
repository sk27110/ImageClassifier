import torch
from tqdm import tqdm
import logging
from torch.utils.tensorboard import SummaryWriter
import torchvision
from datetime import datetime
import os

logger = logging.getLogger("train")

class Trainer:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. 
    –ú–µ—Ç—Ä–∏–∫–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –Ω–∞ —Ç—Ä–µ–π–Ω–µ —Ç–æ–ª—å–∫–æ loss.
    """
    def __init__(self, model, criterion, optimizer, device, train_loader, scheduler, metrics=None, val_loader=None, log_dir=None):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.device = device
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.scheduler = scheduler
        self.metrics = metrics or {}

        self.model.to(self.device)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TensorBoard
        if log_dir is None:
            log_dir = f"runs/experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.writer = SummaryWriter(log_dir)
        logger.info(f"üìä TensorBoard logging to: {log_dir}")

        # –õ–æ–≥–∏—Ä—É–µ–º –≥—Ä–∞—Ñ –º–æ–¥–µ–ª–∏
        self._log_model_graph()

        logger.info(f"üìå Model device: {next(model.parameters()).device}")
        logger.info(f"üìå Batch device: {device}")

    def _log_model_graph(self):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ –≤ TensorBoard"""
        try:
            if self.train_loader is not None:
                # –ë–µ—Ä–µ–º –æ–¥–∏–Ω –±–∞—Ç—á –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞
                sample_batch = next(iter(self.train_loader))
                sample_images = sample_batch["image"][:1].to(self.device)  # –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –¥–ª—è –≥—Ä–∞—Ñ–∞
                self.writer.add_graph(self.model, sample_images)
                logger.info("‚úÖ Model graph logged to TensorBoard")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not log model graph: {e}")

    def train(self, num_epochs=5, patience=7, min_epochs=0):
        """
        –û–±—É—á–µ–Ω–∏–µ —Å Early Stopping
        
        Args:
            num_epochs: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            patience: —Å–∫–æ–ª—å–∫–æ —ç–ø–æ—Ö –∂–¥–∞—Ç—å –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π
            min_epochs: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –ø—Ä–æ–≤–µ—Ä–∫–∏ early stopping
        """
        best_val_loss = float('inf')
        train_losses = []
        val_losses = []
        epochs_without_improvement = 0
        best_epoch = 0
        
        for epoch in range(num_epochs):
            logger.info(f"Epoch {epoch+1}/{num_epochs}")
            
            # 1. –û–±—É—á–∞–µ–º –æ–¥–Ω—É —ç–ø–æ—Ö—É –∏ –ø–æ–ª—É—á–∞–µ–º train loss
            train_loss = self._train_one_epoch(epoch)
            train_losses.append(train_loss)
            
            current_lr = self.optimizer.param_groups[0]['lr']
            logger.info(f"üìä LR: {current_lr:.2e}, Train Loss: {train_loss:.4f}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º train loss –∏ learning rate
            self.writer.add_scalar('Loss/train', train_loss, epoch)
            self.writer.add_scalar('Learning_rate', current_lr, epoch)
            
            # 2. –í–∞–ª–∏–¥–∞—Ü–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
            val_loss = None
            if self.val_loader is not None:
                val_loss, val_metrics = self._validate(epoch)
                val_losses.append(val_loss)
                
                # –õ–æ–≥–∏—Ä—É–µ–º validation loss –∏ –º–µ—Ç—Ä–∏–∫–∏
                self.writer.add_scalar('Loss/val', val_loss, epoch)
                for metric_name, metric_value in val_metrics.items():
                    self.writer.add_scalar(f'Metrics/{metric_name}', metric_value, epoch)
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ train/val loss
                self.writer.add_scalars('Loss_comparison', {
                    'train': train_loss,
                    'val': val_loss
                }, epoch)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_epoch = epoch + 1
                    epochs_without_improvement = 0
                    torch.save(self.model.state_dict(), 'best_model.pth')
                    logger.info(f"üèÜ New best model! Val Loss: {val_loss:.4f}")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –ª—É—á—à–µ–π —ç–ø–æ—Ö–∏
                    if epoch % 5 == 0:  # –ß—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å, –ª–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 5 —É–ª—É—á—à–µ–Ω–∏–π
                        self._log_model_weights(epoch)
                else:
                    epochs_without_improvement += 1
                    logger.info(f"‚è≥ No improvement for {epochs_without_improvement}/{patience} epochs")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ Early Stopping
                if (epoch + 1) >= min_epochs and epochs_without_improvement >= patience:
                    logger.info(f"üõë Early stopping at epoch {epoch+1}. Best val loss: {best_val_loss:.4f} at epoch {best_epoch}")
                    break
                
                # –û–±–Ω–æ–≤–ª—è–µ–º scheduler –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ loss
                if self.scheduler is not None:
                    if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                        self.scheduler.step(val_loss)
                    else:
                        self.scheduler.step()
                        
                    # –õ–æ–≥–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ LR
                    new_lr = self.optimizer.param_groups[0]['lr']
                    if new_lr != current_lr:
                        logger.info(f"üîÑ Learning Rate updated: {new_lr:.2e}")
            
            # 3. –ï—Å–ª–∏ –Ω–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –æ–±–Ω–æ–≤–ª—è–µ–º scheduler –Ω–∞ –æ—Å–Ω–æ–≤–µ train loss
            elif self.scheduler is not None:
                self.scheduler.step()
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
            if epoch % 10 == 0:
                self._log_sample_images(epoch)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if self.val_loader is not None:
            logger.info(f"üéØ Training completed! Best Val Loss: {best_val_loss:.4f} at epoch {best_epoch}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            self.model.load_state_dict(torch.load('best_model.pth', map_location=self.device))
            logger.info("‚úÖ Best model loaded for final evaluation")
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º writer
        self.writer.close()
        logger.info("üìä TensorBoard writer closed")
        
        return train_losses, val_losses

    def _train_one_epoch(self, epoch):
        """–û–±—É—á–∞–µ–º –æ–¥–Ω—É —ç–ø–æ—Ö—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π loss"""
        self.model.train()
        running_loss = 0.0
        total_samples = 0

        for batch_idx, batch in enumerate(tqdm(self.train_loader, desc="Training")):
            images = batch["image"].to(self.device)
            labels = batch["label"].to(self.device)
            
            batch_size = labels.size(0)
            total_samples += batch_size

            outputs = self.model(images)
            loss = self.criterion(outputs, labels)

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            running_loss += loss.item() * batch_size
            
            # –õ–æ–≥–∏—Ä—É–µ–º batch loss –∫–∞–∂–¥—ã–µ 50 –±–∞—Ç—á–µ–π
            if batch_idx % 50 == 0:
                step = epoch * len(self.train_loader) + batch_idx
                self.writer.add_scalar('Loss/train_batch', loss.item(), step)

        avg_loss = running_loss / total_samples
        return avg_loss

    def _validate(self, epoch):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        self.model.eval()
        running_loss = 0.0
        total_samples = 0
        self._reset_metrics()

        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(self.val_loader, desc="Validating")):
                images = batch["image"].to(self.device)
                labels = batch["label"].to(self.device)
                
                batch_size = labels.size(0)
                total_samples += batch_size

                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                running_loss += loss.item() * batch_size

                preds = torch.argmax(outputs, dim=1)
                self._update_metrics(preds, labels)
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
                if epoch % 10 == 0 and batch_idx == 0:
                    self._log_val_images(images[:8], preds[:8], labels[:8], epoch)

        avg_loss = running_loss / total_samples
        metric_results = self._compute_metrics()
        self._print_metrics(avg_loss, metric_results)
        
        return avg_loss, metric_results

    def _log_model_weights(self, epoch):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏"""
        try:
            for name, param in self.model.named_parameters():
                if param.requires_grad and param.numel() > 1:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º bias –∏ —Ç.–¥.
                    self.writer.add_histogram(f'weights/{name}', param, epoch)
                    if param.grad is not None:
                        self.writer.add_histogram(f'grads/{name}', param.grad, epoch)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not log model weights: {e}")

    def _log_sample_images(self, epoch):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞"""
        try:
            if self.train_loader is not None:
                batch = next(iter(self.train_loader))
                images = batch["image"][:8]  # –ë–µ—Ä–µ–º 8 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                grid = torchvision.utils.make_grid(images)
                self.writer.add_image('train_samples', grid, epoch)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not log sample images: {e}")

    def _log_val_images(self, images, predictions, labels, epoch):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏"""
        try:
            # –°–æ–∑–¥–∞–µ–º grid –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            grid = torchvision.utils.make_grid(images)
            self.writer.add_image('val_samples', grid, epoch)
            
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ª–æ–≥–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, confusion matrix
            # –µ—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not log validation images: {e}")

    def _reset_metrics(self):
        for metric in self.metrics.values():
            if hasattr(metric, "reset"):
                metric.reset()

    def _update_metrics(self, preds, labels):
        for metric in self.metrics.values():
            if hasattr(metric, "update"):
                metric.update(preds, labels)

    def _compute_metrics(self):
        results = {}
        for name, metric in self.metrics.items():
            if hasattr(metric, "compute"):
                results[name] = metric.compute().item()
            elif callable(metric):
                results[name] = metric()
            else:
                results[name] = None
        return results

    def _print_metrics(self, loss, metrics_dict):
        metrics_str = " | ".join([f"{k}: {v:.4f}" for k, v in metrics_dict.items()])
        logger.info(f"Val Loss: {loss:.4f} | {metrics_str}")

    def test(self, test_loader):
        """–û—Ç–¥–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        logger.info("üß™ Final testing on test set")
        test_loss, test_metrics = self._evaluate_loader(test_loader, "Test")
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        self.writer.add_scalar('Loss/test_final', test_loss)
        for metric_name, metric_value in test_metrics.items():
            self.writer.add_scalar(f'Metrics/test_{metric_name}', metric_value)
        
        return test_loss, test_metrics

    def _evaluate_loader(self, loader, prefix="Eval"):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –ª—é–±–æ–º –ª–æ–∞–¥–µ—Ä–µ"""
        self.model.eval()
        running_loss = 0.0
        total_samples = 0
        self._reset_metrics()

        with torch.no_grad():
            for batch in tqdm(loader, desc=f"Evaluating {prefix}"):
                images = batch["image"].to(self.device)
                labels = batch["label"].to(self.device)
                
                batch_size = labels.size(0)
                total_samples += batch_size

                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                running_loss += loss.item() * batch_size

                preds = torch.argmax(outputs, dim=1)
                self._update_metrics(preds, labels)

        avg_loss = running_loss / total_samples
        metric_results = self._compute_metrics()
        
        metrics_str = " | ".join([f"{k}: {v:.4f}" for k, v in metric_results.items()])
        logger.info(f"{prefix} Loss: {avg_loss:.4f} | {metrics_str}")
        
        return avg_loss, metric_results