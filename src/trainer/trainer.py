import torch
from tqdm import tqdm
import logging

logger = logging.getLogger("train")

class Trainer:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –µ–¥–∏–Ω—ã–º –ø–æ–¥—Å—á—ë—Ç–æ–º –º–µ—Ç—Ä–∏–∫.
    –ú–µ—Ç—Ä–∏–∫–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è –≤ —Ä–µ–∂–∏–º–µ eval –¥–ª—è train/val/test, —á—Ç–æ–±—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.
    """
    def __init__(self, model, criterion, optimizer, device, train_loader, scheduler, metrics=None, val_loader=None):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.device = device
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.scheduler = scheduler
        self.metrics = metrics or {}

        self.model.to(self.device)

        logger.info(f"üìå Model device: {next(model.parameters()).device}")
        logger.info(f"üìå Batch device: {device}")

    def train(self, num_epochs=5):
        best_val_loss = float('inf')
        best_epoch = 0
        
        for epoch in range(num_epochs):
            logger.info(f"Epoch {epoch+1}/{num_epochs}")
            
            self._train_one_epoch()
            
            current_lr = self.optimizer.param_groups[0]['lr']
            logger.info(f"üìä Current Learning Rate: {current_lr:.2e}")
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            train_metrics = self.evaluate(self.train_loader, prefix="Train")
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if self.val_loader is not None:
                val_metrics = self.evaluate(self.val_loader, prefix="Val")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                if 'loss' in val_metrics and val_metrics['loss'] < best_val_loss:
                    best_val_loss = val_metrics['loss']
                    best_epoch = epoch + 1
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å (–¥–æ–±–∞–≤—å—Ç–µ –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)
                    # torch.save(self.model.state_dict(), f'best_model_epoch_{epoch+1}.pth')
                    logger.info(f"üèÜ New best model! Val Loss: {best_val_loss:.4f}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º scheduler –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–æ—Ç–µ—Ä–∏
                if self.scheduler is not None:
                    if hasattr(self.scheduler, 'step'):
                        # –î–ª—è ReduceLROnPlateau –ø–µ—Ä–µ–¥–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π loss
                        if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                            self.scheduler.step(val_metrics['loss'])
                        else:
                            # –î–ª—è –¥—Ä—É–≥–∏—Ö scheduler'–æ–≤ (StepLR, CosineAnnealing, etc.)
                            self.scheduler.step()
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –Ω–æ–≤—ã–π LR –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
                    new_lr = self.optimizer.param_groups[0]['lr']
                    if new_lr != current_lr:
                        logger.info(f"üîÑ Learning Rate updated: {new_lr:.2e}")
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –æ–±–Ω–æ–≤–ª—è–µ–º scheduler –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –ø–æ—Ç–µ—Ä–∏
            elif self.scheduler is not None:
                if hasattr(self.scheduler, 'step'):
                    self.scheduler.step()
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        if self.val_loader is not None:
            logger.info(f"üéØ Training completed! Best model at epoch {best_epoch} with Val Loss: {best_val_loss:.4f}")

    def _train_one_epoch(self):
        self.model.train()
        running_loss = 0.0

        for batch in tqdm(self.train_loader, desc="Training"):
            images = batch["image"].to(self.device)
            labels = batch["label"].to(self.device)

            outputs = self.model(images)
            loss = self.criterion(outputs, labels)

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            running_loss += loss.item() * labels.size(0)

        avg_loss = running_loss / len(self.train_loader.dataset)
        logger.info(f"Train step Loss: {avg_loss:.4f}")

    def evaluate(self, loader, prefix="Val"):
        """
        –ï–¥–∏–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–¥—Å—á—ë—Ç–∞ –º–µ—Ç—Ä–∏–∫ –≤ —Ä–µ–∂–∏–º–µ eval.
        –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è train/val/test.
        """
        self.model.eval()
        running_loss = 0.0
        self._reset_metrics()

        with torch.no_grad():
            for batch in tqdm(loader, desc=f"Evaluating {prefix}"):
                images = batch["image"].to(self.device)
                labels = batch["label"].to(self.device)

                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                running_loss += loss.item() * labels.size(0)

                preds = torch.argmax(outputs, dim=1)
                self._update_metrics(preds, labels)

        avg_loss = running_loss / len(loader.dataset)
        metric_results = self._compute_metrics()
        metric_results['loss'] = avg_loss  # –î–æ–±–∞–≤–ª—è–µ–º loss –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–µ—Ç—Ä–∏–∫
        self._print_metrics(prefix, avg_loss, metric_results)
        return metric_results

    def _reset_metrics(self):
        for metric in self.metrics.values():
            if hasattr(metric, "reset"):
                metric.reset()

    def _update_metrics(self, preds, labels):
        for metric in self.metrics.values():
            if hasattr(metric, "update"):
                metric.update(preds, labels)

    def _compute_metrics(self):
        results = {}
        for name, metric in self.metrics.items():
            if hasattr(metric, "compute"):
                results[name] = metric.compute().item()
            elif callable(metric):
                results[name] = metric()
            else:
                results[name] = None
        return results

    def _print_metrics(self, mode, loss, metrics_dict):
        metrics_str = " | ".join([f"{k}: {v:.4f}" for k, v in metrics_dict.items() if k != 'loss'])
        logger.info(f"{mode} Loss: {loss:.4f} | {metrics_str}")